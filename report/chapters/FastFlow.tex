\section*{FastFlow Implementation}
The first parallel implementation uses the \texttt{FastFlow C++} library, developed by the \textit{Universities of Pisa} and \textit{Turin}. The library is designed for multi/many-core CPUs and distributed systems. Each application is interpreted as a directed graph, where each node represents a concurrent processing unit in a shared-memory environment. The library provides basic building blocks and high-level patterns for common parallel computation schemes. For this project, we used the \texttt{Farm} parallel building block.

\par This version of the algorithm improves on the sequential approach by parallelizing the computation of each diagonal. Since the elements within a diagonal rely on different row and column vectors, they can be computed independently. This enables efficient use of shared memory, as threads can safely access the same matrix without race conditions. Each thread is assigned an equal (or nearly equal) portion of elements to compute. After all elements of a diagonal are processed, the algorithm advances to the next one. Threads must wait for the completion of the current diagonal before proceeding to the next. A basic pseudocode of the computation of each thread is shown in Algorithm \ref{Algo2}.

\begin{algorithm}
\begin{algorithmic}
% We assume the major diagonal has index 0, so we start at index 1
\FOR{diag = 1 \textbf{to} matrix\_length - 1}
    \STATE chunk\_size = $\lceil diag.length / num\_threads \rceil$
    \STATE Compute(chunk\_size elements)
    \STATE Waits for other threads to finish their part of the diagonal
\ENDFOR
\end{algorithmic}
\caption{Wavefront Pattern for a single thread of the Farm}
\label{Algo2}
\end{algorithm}

\subsection*{The Farm Building Block}
The key challenge in implementing Algorithm \ref{Algo2} is ensuring that threads know when to start processing their part of elements of the next diagonal. To ensure all threads have completed their current task before moving forward, this project employs the Farm high-level pattern. The Farm pattern consists of a pool of \texttt{Workers} coordinated by an \texttt{Emitter}. The \texttt{Emitter} signals the \texttt{Workers} to begin computation on the next diagonal by sending a token, which identifies the specific chunk of elements they have to process. Each \texttt{Worker} uses the token to determine its assigned elements, processes them, and then sends the token back to the \texttt{Emitter} via a feedback channel. Once the \texttt{Emitter} receives all the tokens back, it redistributes them to the \texttt{Worker}s to initiate the next diagonal's computation. Notably, the \textit{Emitter} also acts as a \texttt{Worker}, processing its chunk of elements after dispatching the tasks and before receiving them back. 

\begin{figure}[h]
    \centering\includegraphics[scale=0.35]{img/FastFlow/Farm_graph.drawio.png}
    
    \caption{Graphical representation of the Farm.}
\end{figure}

\subsection*{Static Chunk Size vs Dynamic Chunk Size}
For each diagonal, the length is not always a multiple of the number of nodes available. In such cases, we round up the division to the nearest integer using the standard C++ function \texttt{std::ceil}. However, this can result in the last Worker receiving fewer elements than the others, or in the worst case, some Workers might receive no elements at all and not contribute to the computation.

A possible workaround is to determine the chunk size dynamically, recalculating it each time a task is sent. The dynamic chunk size is computed as:
\[
    dynamic\_chunk\_size = \lceil {remaining\_elements\_to\_send / remaining\_workers\_with\_no\_task}\rceil
\]
Although this approach requires slightly more computation compared to static case, it ensures a more even distribution of elements. However, if the number of elements in a diagonal is smaller than the number of nodes, some nodes will still remain inactive.

Theoretically the dynamic approach seems to be better than the static method, however when comparing the two in our measurements, as Figure \ref{FF_Chunk} shows, using a dynamic chunk size proves slower. This slowdown occurs probably because the overhead of managing communication between all \texttt{Workers} outweighed the marginal decrease in elements per \texttt{Worker}. Therefore, the algorithm defaults to using static chunk sizing, with the dynamic approach available as an optional alternative through the use of the guard \texttt{DYNAMIC\_CHUNK}.

\begin{figure}[h]
    \centering\includegraphics[scale=0.20]{img/FastFlow/Static_vs_Dynamic.drawio.png}
    
    \caption{Example of the distribution of Workers using static and dynamic chunk size.}
\end{figure}

\subsection*{The DiagInfo class}
Each node needs to track the current diagonal it is processing and its length to determine which elements to compute. Instead of requiring each node to update a local copy of these values with each new task, the information is shared in the object of custom type \texttt{DiagInfo}. Only the \texttt{Emitter} has the responsibility to update before starting work on the next diagonal, through the \texttt{PrepareNextDiagonal()} method. The \texttt{Worker}s simply read from it during their computations.

\subsection*{Farm Limitation}
Another potential parallel optimization lies in the dot product computation. Instead of calculating it element by element, the row and column vectors could be divided into segments, with each segment handled by a different thread. The partial results would then be summed to produce the final dot product. While this idea is appealing, it is impractical in our current approach. As the computation progresses, the diagonals become smaller, eventually having fewer elements than available \texttt{Worker}s. For example, the last diagonal has only one element, leaving most \texttt{Worker}s idle. Unfortunately, in \texttt{FastFlow}'s \texttt{Farm} building block, \texttt{Worker}s cannot directly communicate, so coordinating this task would require extensive communication managed by the \texttt{Emitter}, leading to inefficiencies.

\subsection*{Asymptotic Complexity}
Assume the cost of communication between the \texttt{Emitter} and one \texttt{Worker} is a constant $\mathcal{O}(c)$ with $c > 0$, the complexity simplifies to:

\begin{itemize}
    \item \textbf{Complexity of sending tasks to each Worker =} $num\_workers \times \mathcal{O}(num\_workers \times c)$

    \item \textbf{Complexity of computing all elements of one diagonal =} $ \mathcal{O}(num\_workers \times c) + \mathcal{O}(\max{chunk\_size}) = \mathcal{O}(num\_workers \times c) + \mathcal{O}(\lceil n-1 / num\_threads \rceil)$ where $n$ is the length of the matrix.

    \item \textbf{Overall complexity = } $n-1 \times (\mathcal{O}(num\_workers \times c) + \mathcal{O}(\lceil n-1 / num\_threads \rceil) ) =\mathcal{O}((n - 1) \times num\_workers \times c) + \mathcal{O}((n-1)^{2} / num\_threads)$ where $n$ is the length of the matrix.
\end{itemize}

\subsection*{Measurements}
A \texttt{C++} implementation of the \texttt{FastFlow} approach is implemented in \textit{src\_parallel\_fastflow.cpp}. The program takes the matrix size and the number of threads as arguments. By passing $m$ and $n$, the process creates one \texttt{Emitter} and $n - 1$ \texttt{Workers} to handle a matrix of size $m \times m$. When compiling the project using the CMake configuration files, it will generate two executables: one using static chunk sizes and the other using dynamic chunk sizes. Both versions have been tested on the \textit{``spmcluster.unipi.it''} compute cluster with various thread counts and matrix sizes. It is important to note that while each node in the \textit{spmcluster} is advertised as having $32$ cores, it actually consists of $16$ physical cores, with the remaining ones being virtual cores (\textit{hyperthreaded}).
\par 
The speedup graph in Figure \ref{FF_Speedup} demonstrates that using more threads results in greater performance gains. However, the efficiency graph in Figure \ref{FF_Efficiency} reveals that as the number of threads increases, efficiency diminishes, indicating that the overhead associated with managing additional threads becomes more and more large. 

\begin{figure}[h!]
    \centering
    % First row: Two images adjacent
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/FastFlow/ff_static_graph.png}
        \caption{FastFlow Strong Scaling comparison.}
        \label{FF_Strong_scaling}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/FastFlow/ff_static_vs_dynamic.png}
        \caption{FastFlow Weak scaling comparison.}
        \label{FF_Chunk}
    \end{minipage}
    
    % First row: Two images adjacent
    \begin{minipage}[t!]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/FastFlow/ff_speedup.png}
        \caption{FastFlow Speedup comparison.}
        \label{FF_Speedup}
    \end{minipage}
    \hfill
    \begin{minipage}[t!]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/FastFlow/ff_efficiency.png}
        \caption{FastFlow Efficiency comparison.}
        \label{FF_Efficiency}
    \end{minipage}

    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/FastFlow/ff_scalability.png}
        \caption{FastFlow Scalability comparison.}
        \label{FF_Scalability}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/FastFlow/ff_sequential_vs_fastflow.png}
        \caption{Comparison between sequential and FastFlow implementations.}
        \label{FF_Sequential_vs_FastFlow}
    \end{minipage}
\end{figure}